{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLxVyStTp8bl"
      },
      "outputs": [],
      "source": [
        "################# HEALPER PREPROCESS ######################################\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import cv2\n",
        "\n",
        "\n",
        "ALL_DET = \"/content/drive/MyDrive/HOI_Data/All_data/\"\n",
        "INFOS = \"/content/drive/MyDrive/HOI_Data/infos/\"\n",
        "\n",
        "all_data_directory = ALL_DET\n",
        "\n",
        "ann_file_train = all_data_directory + 'Annotations_vcoco/train_annotations.json'\n",
        "ann_file_val = all_data_directory + 'Annotations_vcoco/val_annotations.json'\n",
        "ann_file_test = all_data_directory + 'Annotations_vcoco/test_annotations.json'\n",
        "\n",
        "with open(ann_file_train) as f:\n",
        "    ANNOTATIONS_TRAIN = json.load(f)\n",
        "\n",
        "with open(ann_file_val) as f:\n",
        "    ANNOTATIONS_VAL = json.load(f)\n",
        "\n",
        "with open(ann_file_test) as f:\n",
        "    ANNOTATIONS_TEST = json.load(f)\n",
        "\n",
        "OBJ_path_train = all_data_directory + 'Object_Detections_vcoco/train/'\n",
        "OBJ_path_test = all_data_directory + 'Object_Detections_vcoco/val/'\n",
        "\n",
        "VERB_TO_ID = {\n",
        "    'carry': 0,\n",
        "    'catch': 1,\n",
        "    'cut_instr':2,\n",
        "    'cut_obj': 3,\n",
        "    'drink': 4,\n",
        "    'eat_instr':5,\n",
        "    'eat_obj': 6,\n",
        "    'hit_instr':7,\n",
        "    'hit_obj': 8,\n",
        "    'hold': 9,\n",
        "    'jump': 10,\n",
        "    'kick': 11,\n",
        "    'lay': 12,\n",
        "    'look': 13,\n",
        "    'point': 14,\n",
        "    'read': 15,\n",
        "    'ride': 16,\n",
        "    'run': 17,\n",
        "    'sit': 18,\n",
        "    'skateboard': 19,\n",
        "    'ski': 20,\n",
        "    'smile': 21,\n",
        "    'snowboard': 22,\n",
        "    'stand': 23,\n",
        "    'surf': 24,\n",
        "    'talk_on_phone': 25,\n",
        "    'throw': 26,\n",
        "    'walk': 27,\n",
        "    'work_on_computer': 28\n",
        "}\n",
        "\n",
        "MATCHING_IOU = 0.5\n",
        "NUMBER_OF_VERBS = 29\n",
        "\n",
        "def get_detections(segment_key, flag):\n",
        "\n",
        "    \"\"\"based on threshold score values, for score person and score obj in object detctions, we distinct the object detections. This includes actual co-ordinate of the person and object in the images.\n",
        "\n",
        "    segment_key = train / test / val.\n",
        "    flag = image_no.\n",
        "\n",
        "    Returns:\n",
        "        d_p_boxes = all the presons bbx in the image -> actual image co-ordinates\n",
        "        d_o_boxes = all the objects bbx in the image -> actual image co-ordinates\n",
        "        scores_p = person scores in the image.\n",
        "        scores_o = object scores in the image\n",
        "        class_id_persons = class_id for the person boxes in image.\n",
        "        class_id_objects = class_id for the object boxes in image.\n",
        "        annotation = cleaned up annotation of the form [{'person_box': [0.96, 1.07, 352., 145],\n",
        "          'hois': [{'verb': 'cut_obj', 'obj_box': [117.61, 175.46, 522.51, 332.6]}, {'verb': 'hold', 'obj_box': [163.17, 50.3, 231.19, 116]}]}].\n",
        "        img_shape = shape of this image (W, H)\n",
        "    \"\"\"\n",
        "\n",
        "    SCORE_PER = 0.6\n",
        "    SCORE_OBJ = 0.3\n",
        "    select_threshold=2000000\n",
        "    if flag == 'train':\n",
        "        annotation = ANNOTATIONS_TRAIN[str(segment_key)]\n",
        "        cur_obj_paths = OBJ_path_train + \"COCO_train2014_%.12i.json\" % (segment_key)\n",
        "\n",
        "    elif flag == 'test':\n",
        "        annotation = ANNOTATIONS_TEST[str(segment_key)]\n",
        "        cur_obj_paths = OBJ_path_test + \"COCO_val2014_%.12i.json\" % (segment_key)\n",
        "\n",
        "    elif flag == 'val':\n",
        "        annotation = ANNOTATIONS_VAL[str(segment_key)]\n",
        "        cur_obj_paths = OBJ_path_train + \"COCO_train2014_%.12i.json\" % (segment_key)\n",
        "\n",
        "    annotation = clean_up_annotation(annotation)\n",
        "\n",
        "    with open(cur_obj_paths) as f:\n",
        "        detections = json.load(f)\n",
        "\n",
        "    img_H = detections['H']\n",
        "    img_W = detections['W']\n",
        "    img_shape = [img_W, img_H]\n",
        "    persons_d, objects_d = analyze_detections(detections, SCORE_PER, SCORE_OBJ)\n",
        "    d_p_boxes, scores_p, class_id_persons = get_boxes_det(persons_d, img_H, img_W)\n",
        "    d_o_boxes, scores_o, class_id_objects = get_boxes_det(objects_d, img_H, img_W)\n",
        "\n",
        "    if len(d_p_boxes)>select_threshold:\n",
        "        d_p_boxes,scores_p ,class_id_persons= d_p_boxes[0:select_threshold],scores_p[0:select_threshold],class_id_persons[0:select_threshold]\n",
        "\n",
        "    if len(d_o_boxes)>select_threshold-1:\n",
        "        d_o_boxes,scores_o,class_id_objects= d_o_boxes[0:select_threshold-1],scores_o[0:select_threshold-1],class_id_objects[0:select_threshold-1]\n",
        "\n",
        "    return d_p_boxes, d_o_boxes, scores_p, scores_o, class_id_persons, class_id_objects, annotation, img_shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def analyze_detections(detections, SCORE_PER, SCORE_OBJ):\n",
        "\n",
        "    \"\"\"gives all the person predicitons and the objects in an image seperately.\n",
        "       based on high scores.\n",
        "\n",
        "    Returns:\n",
        "        two lists, one is the persons detected in an image, the other is the objects detected in an image.\n",
        "    \"\"\"\n",
        "\n",
        "    persons = []\n",
        "    objects = []\n",
        "\n",
        "    for det in detections['detections']:\n",
        "        if det['class_str'] == 'person':\n",
        "            if det['score'] >= SCORE_PER:\n",
        "                persons.append(det)\n",
        "\n",
        "        else:\n",
        "            if det['score'] >= SCORE_OBJ:\n",
        "                objects.append(det)\n",
        "\n",
        "    return persons, objects\n",
        "\n",
        "\n",
        "def get_boxes_det(dets, img_H, img_W):\n",
        "\n",
        "    \"\"\"Gives the distinct boxes, scores and classes present in the detection recieved\n",
        "       Gives us the actual co ordinates in the image for the persons and objects.\n",
        "\n",
        "    Args:\n",
        "        dets (_type_): [{'class_str': 'tie','score': 0.063, 'class_no': 28, 'box_coords': [0.06, 0.109, 0.847, 0.535]}]\n",
        "        img_H (_type_): height\n",
        "        img_W (_type_): width\n",
        "\n",
        "    Returns:\n",
        "        type: boxes, scores, class_no. present in the given detection (Person / Object).\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = []\n",
        "    scores = []\n",
        "    class_no = []\n",
        "\n",
        "    for det in dets:\n",
        "        top, left, bottom, right = det['box_coords']\n",
        "        scores.append(det['score'])\n",
        "        class_no.append(det['class_no'])\n",
        "        left, top, right, bottom = left* img_W, top*img_H, right*img_W, bottom*img_H\n",
        "        boxes.append([left, top, right, bottom])\n",
        "\n",
        "    return boxes, scores, class_no\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_up_annotation(annotation):\n",
        "\n",
        "    \"\"\"\n",
        "    Recieves the main annotation structure that is present in the main dataset\n",
        "       for an image, and converts it into more explainable and easily readable annotation\n",
        "       for verb with no objectm, the object bbx remains []\n",
        "\n",
        "    Args:\n",
        "        annotation (_type_):\n",
        "        '106497': [{'person_bbx': [0.96, 1.07, 352., 145],\n",
        "                    'Verbs': 'cut_obj',\n",
        "                    'object': {'obj_bbx': [117.61, 175.46, 522.51, 332.6]}},\n",
        "                   {'person_bbx': [0.96, 1.07, 3525, 145],\n",
        "                    'Verbs': 'hold',\n",
        "                    'object': {'obj_bbx': [163.17, 50.3, 231.19, 116]}}]\n",
        "\n",
        "    Returns:\n",
        "        _type_:\n",
        "        [{'person_box': [0.96, 1.07, 352., 145],\n",
        "          'hois': [{'verb': 'cut_obj', 'obj_box': [117.61, 175.46, 522.51, 332.6]}, {'verb': 'hold', 'obj_box': [163.17, 50.3, 231.19, 116]}]}]\n",
        "    \"\"\"\n",
        "\n",
        "    persons_dict = {}\n",
        "\n",
        "    for hoi in annotation:\n",
        "        box = hoi['person_bbx']\n",
        "        box = [int(coord) for coord in box]\n",
        "        dkey = tuple(box)\n",
        "        objects = hoi['object']\n",
        "\n",
        "        if len(objects['obj_bbx']) == 0:\n",
        "            cur_oi = {\n",
        "                'verb': hoi['Verbs'],\n",
        "                'obj_box': []\n",
        "            }\n",
        "        else:\n",
        "            cur_oi = {\n",
        "                'verb': hoi['Verbs'],\n",
        "                'obj_box': [int(coord) for coord in objects['obj_bbx']]\n",
        "            }\n",
        "        if dkey not in persons_dict:\n",
        "            persons_dict[dkey] = {'person_box': box, 'hois':[cur_oi]}\n",
        "        else:\n",
        "            persons_dict[dkey]['hois'].append(cur_oi)\n",
        "    pers_list = []\n",
        "\n",
        "    for dkey in persons_dict.keys():\n",
        "        pers_list.append(persons_dict[dkey])\n",
        "\n",
        "    return pers_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_compact_detections(segment_key, flag):\n",
        "\n",
        "    \"\"\"This thing is required for building attention maps.\n",
        "       here, we get the numpy array version for the person bboxes.\n",
        "       here it is based on 0 to 1 value. not the image co-ordinate value.\n",
        "       in objects_np, there is an added co-ordinate , this is for no object involved verbs.\n",
        "       and the co-ordinate is 0, 0, 0, 0.\n",
        "       as person class is 0, for this specific object, class is given 0.\n",
        "       it can be seen in class_id_objects, 1st val is 1.\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "\n",
        "    d_p_boxes, d_o_boxes, scores_p, scores_o, class_id_persons, class_id_objects, annotation, img_shape = get_detections(segment_key, flag)\n",
        "\n",
        "    img_W, img_H = img_shape[0], img_shape[1]\n",
        "    no_person_dets = len(d_p_boxes)\n",
        "    no_object_dets = len(d_o_boxes)\n",
        "    persons_np = np.zeros([no_person_dets, 4], np.float32)\n",
        "    objects_np = np.zeros([no_object_dets+1, 4], np.float32)\n",
        "    class_id_objects.insert(0, 1)\n",
        "    if no_person_dets != 0:\n",
        "        persons_np = np.array(d_p_boxes, np.float32)\n",
        "    objects_np = np.array([[0, 0, 0, 0]] + d_o_boxes, np.float32)\n",
        "    persons_np = persons_np / np.array([img_W, img_H, img_W, img_H])\n",
        "    objects_np = objects_np / np.array([img_W, img_H, img_W, img_H])\n",
        "\n",
        "    return {\n",
        "        'person_bbx': persons_np,\n",
        "        'objects_bbx': objects_np,\n",
        "        'person_bbx_score': scores_p,\n",
        "        'object_bbx_score': scores_o,\n",
        "        'class_id_objects': class_id_objects\n",
        "    }\n",
        "\n",
        "\n",
        "def get_attention_maps(segment_key, flag):\n",
        "    \"\"\"it gives us a map type representation for the attentions at the object level and the person level.\n",
        "        we get the data for building the attention map.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        for all the person, object pair, we build the union box, and get the attention map for all pairs.\n",
        "    \"\"\"\n",
        "    compact_detections = get_compact_detections(segment_key, flag)\n",
        "    persons_np, objects_np = compact_detections['person_bbx'], compact_detections['objects_bbx']\n",
        "    union_box = []\n",
        "    no_person_dets = len(persons_np)\n",
        "    no_object_dets = len(objects_np)\n",
        "    for dp_i in range(no_person_dets):\n",
        "        for do_i in range(no_object_dets):\n",
        "            union_box.append(union_BOX(persons_np[dp_i], objects_np[do_i], segment_key))\n",
        "\n",
        "    return np.concatenate(union_box)\n",
        "\n",
        "\n",
        "def union_BOX(roi_pers, roi_objs, segment_key, H=64, W=64):\n",
        "\n",
        "    \"\"\"\n",
        "    this is used for building the attention maps. it is used for getting the map co ordinates with attentions.\n",
        "    \"\"\"\n",
        "    assert H == W\n",
        "    roi_pers = np.array(roi_pers*H, dtype=int)\n",
        "    roi_objs = np.array(roi_objs*H, dtype=int)\n",
        "    sample_box = np.zeros([1, 2, H, W])\n",
        "    sample_box[0, 0, roi_pers[1]:roi_pers[3]+1, roi_pers[0]:roi_pers[2]+1] = 100\n",
        "    sample_box[0, 1, roi_objs[1]:roi_objs[3]+1, roi_objs[0]:roi_objs[2]+1] = 100\n",
        "\n",
        "    return sample_box\n",
        "\n",
        "\n",
        "\n",
        "def get_compact_label(segment_key, flag):\n",
        "\n",
        "    \"\"\"This is where the main fun starts, this compares the annotation detections, with the original object detections, by making them compared with each other, we create the robust annotations. Now, here, we,\n",
        "    throughout the process of filtering, we use IoU. to get correct annotations. Instead of using bbx from annot, we use more robust bbx co-ordinates from object_det with the help of IoU relation.\n",
        "\n",
        "    Returns:\n",
        "       labels_np : this is of size (num_of_persons, num_of_objects+1, num_of_verbs) -> + 1 for verb without objects\n",
        "       labels_single : for every possible, person object pair, we get whether there is verb or not.\n",
        "    \"\"\"\n",
        "\n",
        "    d_p_boxes, d_o_boxes, scores_p, scores_o, class_id_persons, class_id_objects, annotation, img_shape = get_detections(segment_key, flag)\n",
        "\n",
        "    no_person_dets, no_obj_dets = len(d_p_boxes), len(d_o_boxes)\n",
        "    labels_np = np.zeros([no_person_dets, no_obj_dets+1, NUMBER_OF_VERBS], np.int32)\n",
        "\n",
        "    a_p_boxes = [ann['person_box'] for ann in annotation]\n",
        "    iou_mtx = get_iou_mtx(a_p_boxes, d_p_boxes)\n",
        "\n",
        "    if no_obj_dets!=0 and len(a_p_boxes)!=0:\n",
        "        max_iou_each_det = np.max(iou_mtx, axis=0)\n",
        "        index_max_each_det = np.argmax(iou_mtx, axis=0)\n",
        "\n",
        "        for dd in range(no_person_dets):\n",
        "            cur_max_iou = max_iou_each_det[dd]\n",
        "            if cur_max_iou < MATCHING_IOU:\n",
        "                continue\n",
        "            matched_anno = annotation[index_max_each_det[dd]]\n",
        "            hoi_anns = matched_anno['hois']\n",
        "            no_object_hois = [oi for oi in hoi_anns if len(oi['obj_box'])==0]\n",
        "\n",
        "            for no_hoi in no_object_hois:\n",
        "                verb_idx = VERB_TO_ID[no_hoi['verb']]\n",
        "                labels_np[dd, 0, verb_idx] = 1\n",
        "\n",
        "            object_hois = [oi for oi in hoi_anns if len(oi['obj_box'])!=0]\n",
        "            a_o_boxes = [oi['obj_box'] for oi in object_hois]\n",
        "            iou_mtx_o = get_iou_mtx(a_o_boxes, d_o_boxes)\n",
        "\n",
        "            if a_o_boxes and d_o_boxes:\n",
        "                for do in range(len(d_o_boxes)):\n",
        "                    for ao in range(len(a_o_boxes)):\n",
        "                        cur_iou = iou_mtx_o[ao, do]\n",
        "                        if cur_iou < MATCHING_IOU:\n",
        "                            continue\n",
        "                        current_hoi = object_hois[ao]\n",
        "                        verb_idx = VERB_TO_ID[current_hoi['verb']]\n",
        "                        labels_np[dd, do+1, verb_idx] = 1\n",
        "\n",
        "        comp_labels = labels_np.reshape(no_person_dets*(no_obj_dets+1), NUMBER_OF_VERBS)\n",
        "        labels_single=np.array([1 if i.any()==True else 0 for i in comp_labels])\n",
        "        labels_single=labels_single.reshape(np.shape(labels_single)[0],1)\n",
        "        return{'labels_all':labels_np,'labels_single':labels_single}\n",
        "\n",
        "    else:\n",
        "        comp_labels = labels_np.reshape(no_person_dets*(no_obj_dets+1), NUMBER_OF_VERBS)\n",
        "        labels_single=np.array([1 if i.any()==True else 0 for i in comp_labels])\n",
        "        labels_single=labels_single.reshape(np.shape(labels_single)[0],1)\n",
        "        return{'labels_all':labels_np,'labels_single':labels_single}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_iou_mtx(anns, dets):\n",
        "\n",
        "    \"\"\"gives us the 2 d matrix of size (n_ann, n_dets). Containing the IoU values of the bboxes\n",
        "    \"\"\"\n",
        "\n",
        "    no_at = len(anns)\n",
        "    no_dt = len(dets)\n",
        "    iou_mtx = np.zeros([no_at, no_dt])\n",
        "\n",
        "    for at_n in range(no_at):\n",
        "        at_box = anns[at_n]\n",
        "        for dt_n in range(no_dt):\n",
        "            dt_box = dets[dt_n]\n",
        "            iou_mtx[at_n, dt_n] = IoU_box(at_box, dt_box)\n",
        "\n",
        "    return iou_mtx\n",
        "\n",
        "\n",
        "def IoU_box(box1, box2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        box1 : left1, top1, right1, bottom1\n",
        "        box2 : left2, top2, right2, bottom2\n",
        "\n",
        "    returns:\n",
        "        intersection over union\n",
        "    \"\"\"\n",
        "\n",
        "    left1, top1, right1, bottom1 = box1\n",
        "    left2, top2, right2, bottom2 = box2\n",
        "\n",
        "    left_int, top_int = max(left1, left2), max(top1, top2)\n",
        "    right_int, bottom_int = min(right1, right2), min(bottom1, bottom2)\n",
        "\n",
        "    area_intersection = max(0, right_int-left_int) * max(0, bottom_int-top_int)\n",
        "\n",
        "    area1 = (right1 - left1) * (bottom1 - top1)\n",
        "    area2 = (right2 - left2) * (bottom2 - top2)\n",
        "\n",
        "    IoU = area_intersection / (area1+area2 - area_intersection)\n",
        "\n",
        "    return IoU\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_bad_detections(segment_key,flag):\n",
        "\n",
        "    \"\"\"Get detections with no persons.\n",
        "    \"\"\"\n",
        "\n",
        "    labels_all=get_compact_label(segment_key,flag)['labels_all']\n",
        "    if labels_all.size==0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def dry_run():\n",
        "\n",
        "    ALL_SEGS_train = ANNOTATIONS_TRAIN.keys()\n",
        "    ALL_SEGS_val = ANNOTATIONS_VAL.keys()\n",
        "    ALL_SEGS_test = ANNOTATIONS_TEST.keys()\n",
        "\n",
        "    ALL_SEGS_train = [int(v) for v in ALL_SEGS_train]\n",
        "    ALL_SEGS_train.sort()\n",
        "    ALL_SEGS_val = [int(v) for v in ALL_SEGS_val]\n",
        "    ALL_SEGS_val.sort()\n",
        "    new_anns = {}\n",
        "    ALL_SEGS_test = [int(v) for v in ALL_SEGS_test]\n",
        "    ALL_SEGS_test.sort()\n",
        "\n",
        "    bad_detections_train = []\n",
        "    bad_detections_val = []\n",
        "    bad_detections_test = []\n",
        "\n",
        "    ######### detect bad detections #############\n",
        "\n",
        "    for segkey in (ALL_SEGS_train):\n",
        "\n",
        "        if get_bad_detections(segkey, \"train\"):\n",
        "            bad_detections_train.append(segkey)\n",
        "\n",
        "\n",
        "    for segkey in (ALL_SEGS_val):\n",
        "\n",
        "        if get_bad_detections(segkey, \"val\"):\n",
        "            bad_detections_val.append(segkey)\n",
        "\n",
        "\n",
        "    for segkey in (ALL_SEGS_test):\n",
        "\n",
        "        if get_bad_detections(segkey, 'test'):\n",
        "            bad_detections_test.append(segkey)\n",
        "\n",
        "\n",
        "    return bad_detections_train, bad_detections_val, bad_detections_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftSJsDObt69B"
      },
      "outputs": [],
      "source": [
        "############ CAlCULATE AP CLASSWISE #######################################\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "NO_VERBS = 29\n",
        "VERBS_NO_COCO = 80\n",
        "\n",
        "VERB2ID = [\n",
        "    'carry', 'catch', 'cut_instr', 'cut_obj', 'drink', 'eat_instr', 'eat_obj',\n",
        "    'hit_instr', 'hit_obj', 'hold', 'jump', 'kick', 'lay', 'look', 'point',\n",
        "    'read', 'ride', 'run', 'sit', 'skateboard', 'ski', 'smile', 'snowboard',\n",
        "    'stand', 'surf', 'talk_on_phone', 'throw', 'walk', 'work_on_computer'\n",
        "]\n",
        "\n",
        "coco_verbs = [\n",
        "    u'person', u'bicycle', u'car', u'motorcycle', u'airplane', u'bus', u'train',\n",
        "    u'truck', u'boat', u'traffic light', u'fire hydrant', u'stop sign',\n",
        "    u'parking meter', u'bench', u'bird', u'cat', u'dog', u'horse', u'sheep',\n",
        "    u'cow', u'elephant', u'bear', u'zebra', u'giraffe', u'backpack', u'umbrella',\n",
        "    u'handbag', u'tie', u'suitcase', u'frisbee', u'skis', u'snowboard',\n",
        "    u'sports ball', u'kite', u'baseball bat', u'baseball glove', u'skateboard',\n",
        "    u'surfboard', u'tennis racket', u'bottle', u'wine glass', u'cup', u'fork',\n",
        "    u'knife', u'spoon', u'bowl', u'banana', u'apple', u'sandwich', u'orange',\n",
        "    u'broccoli', u'carrot', u'hot dog', u'pizza', u'donut', u'cake', u'chair',\n",
        "    u'couch', u'potted plant', u'bed', u'dining table', u'toilet', u'tv',\n",
        "    u'laptop', u'mouse', u'remote', u'keyboard', u'cell phone', u'microwave',\n",
        "    u'oven', u'toaster', u'sink', u'refrigerator', u'book', u'clock', u'vase',\n",
        "    u'scissors', u'teddy bear', u'hair drier', u'toothbrush'\n",
        "]\n",
        "\n",
        "threshold = 0.1\n",
        "\n",
        "def class_AP(*args):\n",
        "    result = []\n",
        "    predicted_score = args[0]\n",
        "    true_score = args[1]\n",
        "    predicted_single_class = args[2]\n",
        "    true_single_class = args[3]\n",
        "    mean = 0\n",
        "\n",
        "    for k in range(NO_VERBS):\n",
        "        if VERB2ID[k]:\n",
        "            predicted = predicted_score[:, k]\n",
        "            true = true_score[:, k]\n",
        "            try:\n",
        "                AP_s = average_precision_score(true, predicted) * 100\n",
        "            except:\n",
        "                import pdb; pdb.set_trace()\n",
        "\n",
        "            mean += AP_s\n",
        "            result.append((VERB2ID[k], AP_s))\n",
        "\n",
        "    result.append(('Mean', mean / NO_VERBS))\n",
        "    mean = 0.0\n",
        "    counter = 0\n",
        "\n",
        "    return result, [('AP', average_precision_score(true_single_class, predicted_single_class) * 100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4pPq05ApgPm"
      },
      "outputs": [],
      "source": [
        "############ DATA LOADER VCOCO ##########################\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "with open(\"/content/drive/MyDrive/HOI_Data/Checkpoints/bad_detections.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "bad_detections_train = data[\"train\"]\n",
        "bad_detections_val = data[\"val\"]\n",
        "bad_detections_test = data[\"test\"]\n",
        "\n",
        "NO_VERB = 29\n",
        "\n",
        "def vcoco_collate(batch):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        batch (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        image : all images in tensor format, torch.stacked.\n",
        "        labels_all : (B, Total HOI pairs, verb)\n",
        "        labels_single: (B, Total HOI pairs)\n",
        "        image_id : stack of image ids in the batch.\n",
        "        pairs_info : (B, Person no., Obj no., Verb no.)\n",
        "    \"\"\"\n",
        "    image = []\n",
        "    image_id = []\n",
        "    pairs_info = []\n",
        "    labels_all = []\n",
        "    labels_single = []\n",
        "    for index, item in enumerate(batch):\n",
        "        image.append(item['image'])\n",
        "        image_id.append(torch.tensor(int(item['image_id'])))\n",
        "        pairs_info.append(torch.tensor(np.shape(item['labels_all'])))\n",
        "        tot_HOI = int(np.shape(item['labels_single'])[0])\n",
        "        labels_all.append(torch.tensor(item['labels_all'].reshape(tot_HOI, NO_VERB)))\n",
        "        labels_single.append(torch.tensor(item['labels_single']))\n",
        "    return [\n",
        "        torch.stack(image),\n",
        "        torch.cat(labels_all),\n",
        "        torch.cat(labels_single),\n",
        "        torch.stack(image_id),\n",
        "        torch.stack(pairs_info),\n",
        "    ]\n",
        "\n",
        "class Rescale:\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img2 = transform.resize(image, (new_h, new_w))\n",
        "        return img2\n",
        "\n",
        "class ToTensor:\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.from_numpy(image).float()\n",
        "\n",
        "class vcoco_Dataset:\n",
        "    def __init__(self, json_file_image, root_dir, transform=None):\n",
        "        with open(json_file_image) as json_file_:\n",
        "            self.vcoco_frame_file = json.load(json_file_)\n",
        "        self.flag = json_file_image.split('/')[-1].split('_')[0]\n",
        "        if self.flag == 'train':\n",
        "            self.vcoco_frame = [\n",
        "                x for x in self.vcoco_frame_file.keys() if x not in str(bad_detections_train)\n",
        "            ]\n",
        "        elif self.flag == 'val':\n",
        "            self.vcoco_frame = [\n",
        "                x for x in self.vcoco_frame_file.keys() if x not in str(bad_detections_val)\n",
        "            ]\n",
        "        elif self.flag == 'test':\n",
        "            self.vcoco_frame = [\n",
        "                x for x in self.vcoco_frame_file.keys() if x not in str(bad_detections_test)\n",
        "            ]\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vcoco_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.flag == 'test':\n",
        "            img_pre_suffix = 'COCO_val2014_' + str(self.vcoco_frame[idx]).zfill(12) + '.jpg'\n",
        "        else:\n",
        "            img_pre_suffix = 'COCO_train2014_' + str(self.vcoco_frame[idx]).zfill(12) + '.jpg'\n",
        "\n",
        "        all_labels = get_compact_label(int(self.vcoco_frame[idx]), self.flag)\n",
        "        labels_all = all_labels['labels_all']\n",
        "        labels_single = all_labels['labels_single']\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, img_pre_suffix)\n",
        "        ids = [int(self.vcoco_frame[idx]), self.flag]\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'labels_all': labels_all,\n",
        "            'labels_single': labels_single,\n",
        "            'image_id': self.vcoco_frame[idx],\n",
        "        }\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN002XSosSRt"
      },
      "outputs": [],
      "source": [
        "####################### POOL PAIRING ########################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def get_pool_loc(ims, image_id, flag_, size=(7, 7), spatial_scale=1, batch_size=1):\n",
        "\n",
        "    \"\"\"So, here basically, we get tensors for the image area, where the attention features map are extracted.\n",
        "       Also, for object and person, we cut from the main image, and use adaptive pooling and get trensors.\n",
        "\n",
        "    Returns:\n",
        "        pers_out: tensor from the main image for person area.\n",
        "        objs_out: tensor from the main image for object area.\n",
        "        spatial_locs: spatial locations for the persons, objects in each image.\n",
        "        union_box_out: this gives us the attention maps in an image in a tensor version, a list containg for each img.\n",
        "    \"\"\"\n",
        "\n",
        "    spatial_locs = []\n",
        "    union_box_out = []\n",
        "    pers_out = []\n",
        "    objs_out = []\n",
        "\n",
        "    flag = 'train'\n",
        "    max_pool = nn.AdaptiveMaxPool2d(size)\n",
        "\n",
        "    for im in range(batch_size):\n",
        "        this_image = int(image_id[im])\n",
        "\n",
        "        if int(flag_[im][0]) == 0:\n",
        "            flag = 'train'\n",
        "\n",
        "        elif int(flag_[im][0]) == 1:\n",
        "            flag = 'val'\n",
        "\n",
        "        elif int(flag_[im][0]) == 2:\n",
        "            flag = 'test'\n",
        "\n",
        "        a = get_compact_detections(this_image, flag)\n",
        "        roi_pers, roi_objs = a['person_bbx'], a['objects_bbx']\n",
        "        union_box = get_attention_maps(this_image, flag)\n",
        "        union_box_out.append(torch.tensor(union_box).cuda().float())\n",
        "\n",
        "        C, H, W = ims[im].size()[0], ims[im].size()[1], ims[im].size()[2]\n",
        "        spatial_scale = [W, H, W, H]\n",
        "        image_this_im = ims[im]\n",
        "        roi_pers, roi_objs = roi_pers*spatial_scale, roi_objs*spatial_scale\n",
        "\n",
        "        # pooling persons\n",
        "\n",
        "        for index, roi_val in enumerate(roi_pers):\n",
        "            x1, y1, x2, y2 = int(roi_val[0]), int(roi_val[1]), int(roi_val[2]), int(roi_val[3])\n",
        "            sp = [x1, y1, x2, y2, x2-x1, y2-y1]\n",
        "            image = image_this_im.narrow(0, 0, image_this_im.size()[0])[..., y1:(y2+1), x1:(x2+1)]\n",
        "            pooled = max_pool(image)\n",
        "            pers_out.append((pooled))\n",
        "            spatial_locs.append(sp)\n",
        "\n",
        "        # pooling objects\n",
        "\n",
        "        for index, roi_val in enumerate(roi_objs):\n",
        "            x1, y1, x2, y2 = int(roi_val[0]), int(roi_val[1]), int(roi_val[2]), int(roi_val[3])\n",
        "            sp = [x1, y1, x2, y2, x2-x1, y2-y1]\n",
        "            image = image_this_im.narrow(0, 0, image_this_im.size()[0])[..., y1:(y2+1), x1:(x2+1)]\n",
        "            pooled = max_pool(image)\n",
        "            objs_out.append((pooled))\n",
        "            spatial_locs.append(sp)\n",
        "\n",
        "    return torch.stack(pers_out), torch.stack(objs_out), spatial_locs, torch.cat(union_box_out)\n",
        "\n",
        "def extract_spatial(hum_box, obj_box):\n",
        "\n",
        "    \"\"\"Extracts spatial with respect to the relation between the distance and the width, height of object and person.\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "\n",
        "    x1h, y1h, x2h, y2h, wh, hh = float(hum_box[0]), float(hum_box[1]), float(hum_box[2]), float(hum_box[3]), float(hum_box[4]), float(hum_box[5])\n",
        "\n",
        "    x1o, y1o, x2o, y2o, wo, ho = float(obj_box[0]), float(obj_box[1]), float(obj_box[2]), float(obj_box[3]), float(obj_box[4]), float(obj_box[5])\n",
        "\n",
        "    if wh == 0.0:\n",
        "        wh += 1\n",
        "    if hh == 0.0:\n",
        "        hh += 1\n",
        "\n",
        "    diff_x = 0.001 if x1h-x1o == 0 else x1h - x1o\n",
        "    diff_y = 0.001 if y1h-y1o == 0 else y1h - y1o\n",
        "\n",
        "    if wo!=0 and ho!=0:\n",
        "        extract = torch.FloatTensor([diff_x/wo, diff_y/ho, math.log(wh/wo), math.log(hh/ho)])\n",
        "\n",
        "    elif wo == 0 and ho != 0:\n",
        "        extract = torch.FloatTensor([diff_x, diff_y/ho, math.log(wh), math.log(hh/ho)])\n",
        "\n",
        "    elif wo != 0 and ho == 0:\n",
        "        extract = torch.FloatTensor([diff_x/wo, diff_y, math.log(wh/wo), math.log(hh)])\n",
        "\n",
        "    else:\n",
        "        extract = torch.FloatTensor([diff_x, diff_y, math.log(wh), math.log(hh)])\n",
        "\n",
        "    return extract.cuda()\n",
        "\n",
        "\n",
        "def pairing(pers, objs, context, spatial_locs, pairs_info):\n",
        "\n",
        "    \"\"\"This gives us, batch wise, pers, objs and pers_objs batch combined with context and spatial features there.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    pairs_out = []\n",
        "    pers_out = []\n",
        "    objs_out = []\n",
        "\n",
        "    start = 0\n",
        "    start_p = 0\n",
        "    start_o = 0\n",
        "\n",
        "    for batch in range(len(pairs_info)):\n",
        "\n",
        "        this_batch_per = int(pairs_info[batch][0])\n",
        "        this_batch_obj = int(pairs_info[batch][1])\n",
        "        this_batch_len = int(pairs_info[batch][0]+pairs_info[batch][1])\n",
        "\n",
        "        batch_pers, batch_objs = pers[start_p:start_p+this_batch_per], objs[start_o:start_o+this_batch_obj]\n",
        "        batch_context = context[batch]\n",
        "        sp_locs_batch = spatial_locs[start:start+this_batch_len]\n",
        "        sp_locs_pers_batch, sp_locs_objs_batch = sp_locs_batch[0:this_batch_per], sp_locs_batch[this_batch_per:this_batch_per+this_batch_obj]\n",
        "\n",
        "        pers_objs = []\n",
        "\n",
        "        for ind_p, i in enumerate(batch_pers):\n",
        "\n",
        "            for ind_o, j in enumerate(batch_objs):\n",
        "                sp_features = extract_spatial(sp_locs_pers_batch[ind_p], sp_locs_objs_batch[ind_o])\n",
        "                pers_objs.append(torch.cat([i, j, sp_features], 0))\n",
        "\n",
        "        pers_objs_batch = torch.stack(pers_objs)\n",
        "\n",
        "        pairs_out.append(torch.cat([pers_objs_batch, batch_context.repeat(pers_objs_batch.size()[0], 1)], 1))\n",
        "        pers_out.append(batch_pers)\n",
        "        objs_out.append(batch_objs)\n",
        "\n",
        "        start += this_batch_len\n",
        "        start_p += this_batch_per\n",
        "        start_o += this_batch_obj\n",
        "\n",
        "\n",
        "    return torch.cat(pairs_out), torch.cat(pers_out), torch.cat(objs_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85Io4RK5sa50"
      },
      "outputs": [],
      "source": [
        "################# PRED VISSSS################################################\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load directory JSON\n",
        "\n",
        "all_data_dir = ALL_DET\n",
        "\n",
        "# Define paths\n",
        "OBJ_PATH_train_s = all_data_dir + 'Object_Detections_vcoco/train/'\n",
        "OBJ_PATH_test_s = all_data_dir + 'Object_Detections_vcoco/val/'\n",
        "image_dir_train = all_data_dir + 'Data_vcoco/train2014'\n",
        "image_dir_val = all_data_dir + 'Data_vcoco/train2014'\n",
        "image_dir_test = all_data_dir + 'Data_vcoco/val2014'\n",
        "\n",
        "# Verb-to-ID and ID-to-Verb mappings\n",
        "VERB2ID_2 = {\n",
        "    u'carry': 0, u'catch': 1, u'cut_instr': 2, u'cut_obj': 3, u'drink': 4,\n",
        "    u'eat_instr': 5, u'eat_obj': 6, u'hit_instr': 7, u'hit_obj': 8,\n",
        "    u'hold': 9, u'jump': 10, u'kick': 11, u'lay': 12, u'look': 13,\n",
        "    u'point': 14, u'read': 15, u'ride': 16, u'run': 17, u'sit': 18,\n",
        "    u'skateboard': 19, u'ski': 20, u'smile': 21, u'snowboard': 22,\n",
        "    u'stand': 23, u'surf': 24, u'talk_on_phone': 25, u'throw': 26,\n",
        "    u'walk': 27, u'work_on_computer': 28\n",
        "}\n",
        "\n",
        "ID2VERB = {v: k for k, v in VERB2ID_2.items()}\n",
        "\n",
        "# Pandas display options\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.options.display.max_columns = 250\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "def visual(image_id, flag, pairs_info, score_HOI, score_interact, score_obj_box, score_per_box, score_REL, score_HOI_pair, ground_truth):\n",
        "    start = 0\n",
        "\n",
        "    for batch in range(len(image_id)):\n",
        "        this_image = int(image_id[batch])\n",
        "        a = get_compact_detections(this_image, flag)\n",
        "        person_bbxn = a['person_bbx']\n",
        "        obj_bbxn = a['objects_bbx']\n",
        "\n",
        "        this_batch_pers = int(pairs_info[batch][0])\n",
        "        this_batch_objs = int(pairs_info[batch][1])\n",
        "        increment = this_batch_pers * this_batch_objs\n",
        "\n",
        "        ground_truth_this_batch = ground_truth[start:start + increment]\n",
        "        score_HOI_this_batch = score_HOI[start:start + increment]\n",
        "        start += increment\n",
        "\n",
        "        if flag == 'train':\n",
        "            cur_obj_path_s = OBJ_PATH_train_s + \"COCO_train2014_%.12i.json\" % this_image\n",
        "            image_dir_s = image_dir_train + '/COCO_train2014_%.12i.jpg' % this_image\n",
        "        elif flag == 'test':\n",
        "            cur_obj_path_s = OBJ_PATH_test_s + \"COCO_val2014_%.12i.json\" % this_image\n",
        "            image_dir_s = image_dir_test + '/COCO_val2014_%.12i.jpg' % this_image\n",
        "        elif flag == 'val':\n",
        "            cur_obj_path_s = OBJ_PATH_train_s + \"COCO_train2014_%.12i.json\" % this_image\n",
        "            image_dir_s = image_dir_val + '/COCO_train2014_%.12i.jpg' % this_image\n",
        "\n",
        "        with open(cur_obj_path_s) as fp:\n",
        "            detections = json.load(fp)\n",
        "\n",
        "        img_H = detections['H']\n",
        "        img_W = detections['W']\n",
        "\n",
        "        person_bbx = np.array([img_W, img_H, img_W, img_H], dtype=float) * person_bbxn\n",
        "        obj_bbx = np.array([img_W, img_H, img_W, img_H], dtype=float) * obj_bbxn\n",
        "        img = cv2.imread(image_dir_s, 3)\n",
        "        img_temp = img\n",
        "\n",
        "        start_index = 0\n",
        "        for person_box in person_bbx:\n",
        "            for object_box in obj_bbx:\n",
        "                img_temp = img.copy()\n",
        "                ground_truth_this_sample = ground_truth_this_batch[start_index]\n",
        "                score_HOI_this_sample = score_HOI_this_batch[start_index]\n",
        "\n",
        "                print(score_HOI_this_sample)\n",
        "\n",
        "                pred = [\n",
        "                    ('GROUND_TRUTH', [(ID2VERB[ind], float(\"%.2f\" % ground_truth_this_sample[ind])) for ind in np.argsort(ground_truth_this_sample)[-5:][::-1]])\n",
        "                ]\n",
        "                pred.append(\n",
        "                    ('TOTAL_PREDICTION', [(ID2VERB[ind], float(\"%.2f\" % score_HOI_this_sample[ind])) for ind in np.argsort(score_HOI_this_sample)[-5:][::-1]])\n",
        "                )\n",
        "\n",
        "                prediction = pd.DataFrame(pred, columns=['Name', 'Prediction'])\n",
        "\n",
        "                x, y, w, h = int(person_box[0]), int(person_box[1]), int(person_box[2] - person_box[0]), int(person_box[3] - person_box[1])\n",
        "                cv2.rectangle(img_temp, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "\n",
        "                x, y, w, h = int(object_box[0]), int(object_box[1]), int(object_box[2] - object_box[0]), int(object_box[3] - object_box[1])\n",
        "                cv2.rectangle(img_temp, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
        "\n",
        "                print('\\nPredictions (Five Highest Confidence Class):\\n{}'.format(prediction))\n",
        "\n",
        "                cv2_imshow(img_temp)\n",
        "                start_index += 1\n",
        "\n",
        "                k = cv2.waitKey(0)\n",
        "                if k == 27:  # ESC key to exit\n",
        "                    cv2.destroyAllWindows()\n",
        "\n",
        "            if k == 27:  # ESC key to exit\n",
        "                cv2.destroyAllWindows()\n",
        "\n",
        "        if k == 27:  # ESC key to exit\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfTTNUjlshc2"
      },
      "outputs": [],
      "source": [
        "##### This script will refine the predictions based on detected object by the object detector. Following by the work of https://github.com/vt-vl-lab/iCAN#######\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Open the pickle file in binary mode and specify the encoding\n",
        "with open(INFOS+'prior.pickle', 'rb') as fp:\n",
        "    priors = pickle.load(fp, encoding='latin1')  # Add encoding='latin1'\n",
        "\n",
        "def apply_prior(Object, prediction_HOI_in):\n",
        "    prediction_HOI = np.ones(prediction_HOI_in.shape)\n",
        "\n",
        "    for index, prediction in enumerate(prediction_HOI):\n",
        "        prediction_HOI[index] = priors[int(Object[index])]\n",
        "\n",
        "    return prediction_HOI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnVcwRw_t7kG"
      },
      "outputs": [],
      "source": [
        "################ PROPER INFERENCE #####################################\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "all_data_dir = ALL_DET\n",
        "\n",
        "OBJ_PATH_train_s = all_data_dir + 'Object_Detections_vcoco/train/'\n",
        "OBJ_PATH_test_s = all_data_dir + 'Object_Detections_vcoco/val/'\n",
        "number_of_roles = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2]\n",
        "\n",
        "proper_keys = [\n",
        "    'carry_agent', 'carry_obj', 'catch_agent', 'catch_obj', 'cut_agent', 'cut_instr', 'cut_agent', 'cut_obj',\n",
        "    'drink_agent', 'drink_instr', 'eat_agent', 'eat_instr', 'eat_agent', 'eat_obj', 'hit_agent', 'hit_instr',\n",
        "    'hit_agent', 'hit_obj', 'hold_agent', 'hold_obj', 'jump_agent', 'jump_instr', 'kick_agent', 'kick_obj',\n",
        "    'lay_agent', 'lay_instr', 'look_agent', 'look_obj', 'point_agent', 'point_instr', 'read_agent', 'read_obj',\n",
        "    'ride_agent', 'ride_instr', 'run_agent', 'sit_agent', 'sit_instr', 'skateboard_agent', 'skateboard_instr',\n",
        "    'ski_agent', 'ski_instr', 'smile_agent', 'snowboard_agent', 'snowboard_instr', 'stand_agent', 'surf_agent',\n",
        "    'surf_instr', 'talk_on_phone_agent', 'talk_on_phone_instr', 'throw_agent', 'throw_obj', 'walk_agent',\n",
        "    'work_on_computer_agent', 'work_on_computer_instr'\n",
        "]\n",
        "\n",
        "def infer_format(image_id, all_scores_batch, flag, all_detections, pairs_info):\n",
        "\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Returns:\n",
        "        The output is like:\n",
        "        all_detections = [{'image_id: 233...', 'person_bbx': [0.3, 0.2, 0.5, 0.5], 'carry_agent':0.45, 'carry_obj':[0.2, 0.3, 0.1, 0.3, 0.7 <- action score last aer ta], .....,  'work_on_computer_instr'}]\n",
        "    \"\"\"\n",
        "    this_batch_start = 0\n",
        "    for batch in range(len(image_id)):\n",
        "        this_image = int(image_id[batch])\n",
        "        persons = all_scores_batch[this_image, 'pers_bbx']\n",
        "        objects = all_scores_batch[this_image, 'obj_bbx']\n",
        "        hum_scores = 0\n",
        "        this_batch_pers = int(pairs_info[batch][0])\n",
        "        this_batch_objs = int(pairs_info[batch][1])\n",
        "        increment = this_batch_pers * this_batch_objs\n",
        "        all_scores = all_scores_batch[this_image, 'score']\n",
        "\n",
        "        if flag == 'train':\n",
        "            cur_obj_path_s = OBJ_PATH_train_s + \"COCO_train2014_%.12i.json\" % (this_image)\n",
        "        elif flag == 'test':\n",
        "            cur_obj_path_s = OBJ_PATH_test_s + \"COCO_val2014_%.12i.json\" % (this_image)\n",
        "        elif flag == 'val':\n",
        "            cur_obj_path_s = OBJ_PATH_train_s + \"COCO_train2014_%.12i.json\" % (this_image)\n",
        "\n",
        "        with open(cur_obj_path_s) as fp:\n",
        "            detections = json.load(fp)\n",
        "\n",
        "        persons_score = []\n",
        "        objects_score = []\n",
        "        objects_score.append(float(1))\n",
        "        number_of_objects = len(objects)\n",
        "        persons_score = np.array(persons_score, dtype=float)\n",
        "        objects_score = np.array(objects_score, dtype=float)\n",
        "        img_H = detections['H']\n",
        "        img_W = detections['W']\n",
        "        index_person = 0\n",
        "        infer_dict = {}\n",
        "\n",
        "        for item_no, role_ids in enumerate((all_scores)):\n",
        "            person_bbxn = persons[item_no]\n",
        "            obj_bbxn = objects[item_no]\n",
        "            person_bbx = np.array([img_W, img_H, img_W, img_H], dtype=float) * person_bbxn\n",
        "            obj_bbx = np.array([img_W, img_H, img_W, img_H], dtype=float) * obj_bbxn\n",
        "            infer_dict = {}\n",
        "\n",
        "            infer_dict['person_box'] = person_bbx.tolist()\n",
        "            infer_dict['image_id'] = this_image\n",
        "            dict_index = 0\n",
        "\n",
        "            for index, k in enumerate(role_ids):\n",
        "                person_action_score = k  # *person_confidence\n",
        "                instances = number_of_roles[index]\n",
        "                for j in range(instances):\n",
        "                    if proper_keys[dict_index + j][-5:] == 'agent':\n",
        "                        agent_key = proper_keys[dict_index + j]\n",
        "\n",
        "                        if agent_key in infer_dict.keys():\n",
        "                            if k > infer_dict[agent_key]:\n",
        "                                infer_dict[agent_key] = float(person_action_score)\n",
        "                        else:\n",
        "                            infer_dict[agent_key] = float(person_action_score)\n",
        "                    else:\n",
        "                        obj_score = k\n",
        "                        obj_bbx_score = np.append(obj_bbx, obj_score)\n",
        "                        infer_dict[proper_keys[dict_index + j]] = obj_bbx_score.tolist()\n",
        "\n",
        "                dict_index += number_of_roles[index]\n",
        "\n",
        "            all_detections.append(infer_dict)\n",
        "\n",
        "    return all_detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5PlrJVJt-Hd"
      },
      "outputs": [],
      "source": [
        "############################# MODEL ###############################################\n",
        "\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet152_Weights\n",
        "\n",
        "\n",
        "lin_size = 1024\n",
        "ids = 80\n",
        "context_size = 1024\n",
        "sp_size = 1024\n",
        "mul = 3\n",
        "deep = 512\n",
        "pool_size = (10, 10)\n",
        "pool_size_pose = (18, 5, 5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size()[0], -1)\n",
        "\n",
        "\n",
        "class HOI_Detector(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "        model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
        "        self.flatten = Flatten()\n",
        "        self.Conv_pretrain = nn.Sequential(*list(model.children())[0:7])\n",
        "\n",
        "\n",
        "        ##### Conv blocks for humans, objects and context #########################\n",
        "\n",
        "        self.Conv_people = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "        self.Conv_objects = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=False),\n",
        "        )\n",
        "\n",
        "        self.Conv_context = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "            nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "        ###################################################################\n",
        "\n",
        "\n",
        "        ########## Attention Feature Model ###########\n",
        "\n",
        "        self.conv_sp_map = nn.Sequential(\n",
        "            nn.Conv2d(2, 64, kernel_size=(5, 5)),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(64, 32, kernel_size=(5, 5)),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.AvgPool2d((13, 13), padding=0, stride=(1, 1))\n",
        "        )\n",
        "\n",
        "        self.spmap_up = nn.Sequential(\n",
        "            nn.Linear(32, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #############################################\n",
        "\n",
        "        ###### Prediction model for attention features #######################\n",
        "\n",
        "        self.lin_spmap_tail = nn.Sequential(\n",
        "            nn.Linear(512, 29)\n",
        "        )\n",
        "\n",
        "        ######################################################################\n",
        "\n",
        "\n",
        "        ###### Graph Model basic Structure ##################################\n",
        "\n",
        "        self.peo_to_obj_w = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.obj_to_peo_w = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        ####################################################################\n",
        "\n",
        "        ############### Interaction prediction model for visual feature ###########################\n",
        "\n",
        "        self.lin_single_head = nn.Sequential(\n",
        "            nn.Linear(lin_size*3+4, 1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.lin_single_tail = nn.Sequential(\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        ######################### Prediction Model for visual features ######################################\n",
        "\n",
        "        self.lin_visual_head = nn.Sequential(\n",
        "            nn.Linear(lin_size*3+4, 1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.lin_visual_tail = nn.Sequential(\n",
        "            nn.Linear(512, 29)\n",
        "        )\n",
        "\n",
        "        #####################################################################################################\n",
        "\n",
        "        ######################### Prediciton Model for graph features #######################################\n",
        "\n",
        "        self.lin_graph_head = nn.Sequential(\n",
        "            nn.Linear(lin_size*2, 1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.lin_graph_tail = nn.Sequential(\n",
        "            nn.Linear(512, 29)\n",
        "        )\n",
        "\n",
        "        #####################################################################################################\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, pairs_info, pairs_info_augmented, image_id, flag_, phase):\n",
        "\n",
        "        out1 = self.Conv_pretrain(x)\n",
        "\n",
        "        rois_people, rois_objects, spatial_locs, union_box = get_pool_loc(out1, image_id, flag_, size=pool_size, spatial_scale=25, batch_size=len(pairs_info))\n",
        "\n",
        "        #### Defining the pooling operations ####\n",
        "\n",
        "        x, y = out1.size()[2], out1.size()[3]\n",
        "        hum_pool = nn.AvgPool2d(pool_size, padding=0, stride=(1, 1))\n",
        "        obj_pool = nn.AvgPool2d(pool_size, padding=0, stride=(1, 1))\n",
        "        context_pool = nn.AvgPool2d((x, y), padding=0, stride=(1, 1))\n",
        "\n",
        "        # Human ->\n",
        "        residual_people = rois_people\n",
        "        res_people = self.Conv_people(rois_people) + residual_people\n",
        "        res_av_people = hum_pool(res_people)\n",
        "        out2_people = self.flatten(res_av_people)\n",
        "        ##########\n",
        "\n",
        "        # Object ->\n",
        "        residual_object = rois_objects\n",
        "        res_object = self.Conv_objects(rois_objects) + residual_object\n",
        "        res_av_object = obj_pool(res_object)\n",
        "        out2_objects = self.flatten(res_av_object)\n",
        "\n",
        "        # Context ->\n",
        "        residual_context = out1\n",
        "        res_context = self.Conv_context(out1) + residual_context\n",
        "        res_av_context = context_pool(res_context)\n",
        "        out2_context = self.flatten(res_av_context)\n",
        "\n",
        "\n",
        "        # Attention feature ->\n",
        "        out2_union = self.spmap_up(self.flatten(self.conv_sp_map(union_box)))\n",
        "\n",
        "        #####################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################ Making Essential Pairing ###################################\n",
        "\n",
        "        pairs, people, objects_only = pairing(out2_people, out2_objects, out2_context, spatial_locs, pairs_info)\n",
        "\n",
        "        ##############################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ############### Interaction probability ######################################\n",
        "\n",
        "        lin_single_h = self.lin_single_head(pairs)\n",
        "        lin_single_t = lin_single_h * out2_union\n",
        "        lin_single = self.lin_single_tail(lin_single_t)\n",
        "        interaction_prob = self.sigmoid(lin_single)\n",
        "\n",
        "        ##############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ####################### Graph Model Base Structure ###################################\n",
        "\n",
        "        people_t = people\n",
        "        objects_only = objects_only\n",
        "        combine_g = []\n",
        "        people_f = []\n",
        "        objects_f = []\n",
        "        pairs_f = []\n",
        "        start_p = 0\n",
        "        start_o = 0\n",
        "        start_c = 0\n",
        "\n",
        "\n",
        "        for batch_num, l in enumerate(pairs_info):\n",
        "\n",
        "            ########### Slicing ######################\n",
        "            people_this_batch = people_t[start_p:start_p+int(l[0])]\n",
        "            no_peo = len(people_this_batch)\n",
        "            objects_this_batch = objects_only[start_o:start_o+int(l[1])][1:]\n",
        "            no_objects_this_batch = objects_only[start_o:start_o+int(l[1])][0]\n",
        "            no_obj = len(objects_this_batch)\n",
        "            interaction_prob_this_batch = interaction_prob[start_c:start_c+int(l[1])*int(l[0])]\n",
        "\n",
        "\n",
        "            if no_obj == 0:\n",
        "                people_this_batch_r = people_this_batch\n",
        "                objects_this_batch_r = no_objects_this_batch.view([1, 1024])\n",
        "\n",
        "            else:\n",
        "                peo_to_obj_this_batch = torch.stack([torch.cat((i, j)) for ind_p, i in enumerate(people_this_batch) for ind_o, j in enumerate(objects_this_batch)])\n",
        "\n",
        "                obj_to_peo_this_batch = torch.stack([torch.cat((i, j)) for ind_p, i in enumerate(objects_this_batch) for ind_o, j in enumerate(people_this_batch)])\n",
        "\n",
        "            #########################################\n",
        "\n",
        "\n",
        "            ########## Adjacency #####################\n",
        "                adj_l = []\n",
        "                adj_po = torch.zeros([no_peo, no_obj]).cpu()\n",
        "                adj_op = torch.zeros([no_obj, no_peo]).cpu()\n",
        "\n",
        "                for index_prob, probs in enumerate(interaction_prob_this_batch):\n",
        "                    if index_prob % (no_obj+1) != 0:\n",
        "                        adj_l.append(probs)\n",
        "\n",
        "                adj_po = torch.cat(adj_l).view(len(adj_l), 1)\n",
        "                adj_op = adj_po\n",
        "\n",
        "            ######### Finding out refined graph features ##########\n",
        "\n",
        "                people_this_batch_r = people_this_batch + torch.mm(adj_po.view([no_peo, no_obj]), self.peo_to_obj_w(objects_this_batch))\n",
        "\n",
        "                objects_this_batch_r = objects_this_batch + torch.mm(adj_op.view([no_obj, no_peo]), self.obj_to_peo_w(people_this_batch))\n",
        "\n",
        "                objects_this_batch_r = torch.cat((no_objects_this_batch.view([1, 1024]), objects_this_batch_r))\n",
        "\n",
        "            #######################################################\n",
        "\n",
        "            #### Restructuring ######\n",
        "            people_f.append(people_this_batch_r)\n",
        "            people_t_f = people_this_batch_r\n",
        "            objects_f.append(objects_this_batch_r)\n",
        "            objects_t_f = objects_this_batch_r\n",
        "\n",
        "            pairs_f.append(torch.stack([torch.cat((i, j)) for ind_p, i in enumerate(people_t_f) for ind_o, j in enumerate(objects_t_f)]))\n",
        "\n",
        "\n",
        "            start_p += int(l[0])\n",
        "            start_o += int(l[1])\n",
        "            start_c += int(l[0]) * int(l[1])\n",
        "\n",
        "\n",
        "\n",
        "        people_graph = torch.cat(people_f)\n",
        "        objects_graph = torch.cat(objects_f)\n",
        "        pairs_graph = torch.cat(pairs_f)\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ########## Prediction from visual features #################\n",
        "\n",
        "        lin_h = self.lin_visual_head(pairs)\n",
        "        lin_t = lin_h * out2_union\n",
        "        lin_visual = self.lin_visual_tail(lin_t)\n",
        "\n",
        "        ############################################################\n",
        "\n",
        "\n",
        "        ########### Prediction from graph features ##################\n",
        "\n",
        "        lin_graph_h = self.lin_graph_head(pairs_graph)\n",
        "        lin_graph_t = lin_graph_h * out2_union\n",
        "        lin_graph = self.lin_graph_tail(lin_graph_t)\n",
        "\n",
        "\n",
        "        ########## Prediction from attention features ##############\n",
        "\n",
        "        lin_att = self.lin_spmap_tail(out2_union)\n",
        "\n",
        "        ############################################################\n",
        "\n",
        "        return [lin_visual, lin_single, lin_graph, lin_att]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rOzjp7guRd-"
      },
      "outputs": [],
      "source": [
        "######################### TRAIN TEST ###################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import errno\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import shutil\n",
        "import json\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "### Fixing seeds ####\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "##########################\n",
        "\n",
        "###################  parameters for person to object class mapping #############################\n",
        "\n",
        "SCORE_TH = 0.6\n",
        "SCORE_OBJ = 0.3\n",
        "epoch_to_change = 400\n",
        "thres_hold = -1\n",
        "\n",
        "###############################################################################################\n",
        "\n",
        "############# Loss function defination ########################################################\n",
        "\n",
        "loss_com = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "loss_com_class = nn.BCEWithLogitsLoss(reduction='none')\n",
        "loss_com_combine = nn.BCELoss(reduction='none')\n",
        "loss_com_single = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "###############################################################################################\n",
        "\n",
        "no_of_classes = 29\n",
        "\n",
        "##### Helper function ######\n",
        "\n",
        "#### Fixing the seeds for all threads ###########\n",
        "\n",
        "def _init_fn(worker_id):\n",
        "    np.random.seed(int(seed))\n",
        "\n",
        "\n",
        "################################################\n",
        "\n",
        "############# Extending number of people #####################################################\n",
        "\n",
        "def extend(inputt, extend_number):\n",
        "\n",
        "    res = np.zeros([1, np.shape(inputt)[-1]])\n",
        "\n",
        "    for a in inputt:\n",
        "        x = np.repeat(a.reshape(1, np.shape(inputt)[-1]), extend_number, axis=0)\n",
        "        res = np.concatenate([res, x], axis=0)\n",
        "\n",
        "    return res[1:]\n",
        "\n",
        "\n",
        "def extend_object(inputt, extend_number):\n",
        "\n",
        "    res = np.zeros([1, np.shape(inputt)[-1]])\n",
        "\n",
        "    x = np.array(inputt.tolist()*extend_number)\n",
        "    res = np.concatenate([res, x], axis=0)\n",
        "\n",
        "    return res[1:]\n",
        "\n",
        "\n",
        "################################# Filtering the results for preparing the output as per V-COCO ######################\n",
        "\n",
        "def filtering(predicted_HOI, true, persons_np, objects_np, filters, pairs_info, image_id):\n",
        "\n",
        "    res1 = np.zeros([1, no_of_classes])\n",
        "    res2 = np.zeros([1, no_of_classes])\n",
        "    res3 = np.zeros([1, no_of_classes])\n",
        "    res4 = np.zeros([1, 4])\n",
        "    res5 = np.zeros([1, 4])\n",
        "    dict_1 = {}\n",
        "    a = 0\n",
        "\n",
        "    increment = [int(i[0] * i[1]) for i in pairs_info]\n",
        "    start = 0\n",
        "\n",
        "    for index, i in enumerate(filters):\n",
        "\n",
        "        res1 = np.concatenate([res1, predicted_HOI[index].reshape(1, no_of_classes)], axis=0)\n",
        "        res2 = np.concatenate([res2, true[index].reshape(1, no_of_classes)], axis=0)\n",
        "        res3 = np.concatenate([res3, predicted_HOI[index].reshape(1, no_of_classes)], axis=0)\n",
        "        res4 = np.concatenate([res4, persons_np[index].reshape(1, 4)], axis=0)\n",
        "        res5 = np.concatenate([res5, objects_np[index].reshape(1, 4)], axis=0)\n",
        "\n",
        "        if index == start + increment[a] - 1:\n",
        "\n",
        "            dict_1[int(image_id[a]), 'score'] = res3[1:]\n",
        "            dict_1[int(image_id[a]), 'pers_bbx'] = res4[1:]\n",
        "            dict_1[int(image_id[a]), 'obj_bbx'] = res5[1:]\n",
        "            res3 = np.zeros([1, no_of_classes])\n",
        "            res4 = np.zeros([1, 4])\n",
        "            res5 = np.zeros([1, 4])\n",
        "            start += increment[a]\n",
        "            a += 1\n",
        "\n",
        "    return dict_1\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "########### Saving Checkpoint ##########################\n",
        "\n",
        "def save_checkpoint(state, filename = 'checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "######################################################\n",
        "\n",
        "########## LIS function from https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network #####################\n",
        "\n",
        "def LIS(x, T, k, w):\n",
        "    return T/(1+np.exp(k-w*x))\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def train_test(model, optimizer, scheduler, dataloader, number_of_epochs, break_point, saving_epoch, folder_name, batch_size, infr, start_epoch, mean_best, visualize):\n",
        "\n",
        "    ######################## Creating the folder where the result will be stored ######################################\n",
        "    try:\n",
        "        os.mkdir(folder_name)\n",
        "\n",
        "    except OSError as exc:\n",
        "        if exc.errno != errno.EEXIST:\n",
        "            raise\n",
        "        pass\n",
        "\n",
        "    file_name = folder_name + '/' + 'result.pickle'\n",
        "\n",
        "    ###################################################################################################################\n",
        "\n",
        "    loss_epoch_train = []\n",
        "    loss_epoch_val = []\n",
        "    loss_epoch_test = []\n",
        "    initial_time = time.time()\n",
        "    result = []\n",
        "\n",
        "    ############## Freeing out the cache memories from cpu and gpus and declaring the phases ##########################\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    if infr == 't' and visualize == 'f': ###### If running from a pretrained model for saving the best result #########\n",
        "        start_epoch = start_epoch - 1\n",
        "        phases = ['test']\n",
        "        end_of_epochs = start_epoch + 1\n",
        "        print(\"Only doing testing for storing results from a model\")\n",
        "\n",
        "    elif visualize != 'f':\n",
        "        if visualize not in phases:\n",
        "            print(\"Error! Asked to show visualization from a unknown set. The choice should be among train, val, test\")\n",
        "            return\n",
        "\n",
        "        else:\n",
        "            phases = [visualize]\n",
        "            end_of_epochs = start_epoch + 1\n",
        "            print(\"only showing predictions from a model\")\n",
        "\n",
        "    else:\n",
        "        end_of_epochs = start_epoch + number_of_epochs\n",
        "\n",
        "    ###################################################################################################################\n",
        "\n",
        "    ############################ Starting the epochs #################################################################\n",
        "\n",
        "    for epoch in range(start_epoch, end_of_epochs):\n",
        "\n",
        "        scheduler.step()\n",
        "        print('Epoch {}/{}'.format(epoch+1,end_of_epochs))\n",
        "        print('-' * 10)\n",
        "        initial_time_epoch = time.time()\n",
        "\n",
        "        for phase in phases:\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "\n",
        "            elif phase == 'val':\n",
        "                model.train()\n",
        "\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "\n",
        "            print('In {}'.format(phase))\n",
        "\n",
        "            detections_train = []\n",
        "            detections_val = []\n",
        "            detections_test = []\n",
        "\n",
        "            true_scores_class = np.ones([1, 80], dtype=int)\n",
        "            true_scores = np.ones([1, 29], dtype=int)\n",
        "            true_scores_single = np.ones([1, 1], dtype=int)\n",
        "            predicted_scores = np.ones([1, 29], dtype=float)\n",
        "            predicted_scores_single = np.ones([1, 1], dtype=float)\n",
        "            predicted_scores_class = np.ones([1, 80], dtype=float)\n",
        "            acc_epoch = 0\n",
        "            iteration = 1\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            ########### Starting the iterations ###################################################################\n",
        "\n",
        "            for iter, i in enumerate(tqdm(dataloader[phase])):\n",
        "\n",
        "                if iter % 20 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                inputs = i[0].to(device)\n",
        "                labels = i[1].to(device)\n",
        "                labels_single = i[2].to(device)\n",
        "                image_id = i[3]\n",
        "                pairs_info = i[4]\n",
        "                min_batch_size = len(pairs_info)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if phase == 'train':\n",
        "                    nav = torch.tensor([[0, epoch]]*min_batch_size).to(device)\n",
        "\n",
        "                elif phase == 'val':\n",
        "                    nav = torch.tensor([[1, epoch]]*min_batch_size).to(device)\n",
        "\n",
        "                else:\n",
        "                    nav = torch.tensor([[2, epoch]]*min_batch_size).to(device)\n",
        "\n",
        "                true = (labels.data).cpu().numpy()\n",
        "                true_single = (labels_single.data).cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "                with torch.set_grad_enabled(phase=='train' or phase=='val'):\n",
        "\n",
        "                    model_out = model(inputs, pairs_info, pairs_info, image_id, nav, phase)\n",
        "                    outputs = model_out[0]         ## Visual\n",
        "                    outputs_single = model_out[1]\n",
        "                    outputs_combine = model_out[2] ## graph\n",
        "                    outputs_gem = model_out[3]     ## attention\n",
        "\n",
        "                    predicted_HOI = sigmoid(outputs).data.cpu().numpy()\n",
        "                    predicted_HOI_combine = sigmoid(outputs_combine).data.cpu().numpy()\n",
        "                    predicted_single = sigmoid(outputs_single).data.cpu().numpy()\n",
        "                    predicted_gem = sigmoid(outputs_gem).data.cpu().numpy()\n",
        "                    predicted_HOI_pair = predicted_HOI\n",
        "\n",
        "\n",
        "                    start_index = 0\n",
        "                    start_obj = 0\n",
        "                    start_pers = 0\n",
        "                    start_tot = 0\n",
        "                    pers_index = 1\n",
        "                    persons_score_extended = np.zeros([1, 1])\n",
        "                    objects_score_extended = np.zeros([1, 1])\n",
        "                    class_ids_extended = np.zeros([1, 1])\n",
        "                    persons_np_extended = np.zeros([1, 4])\n",
        "                    objects_np_extended = np.zeros([1, 4])\n",
        "                    start_no_obj = 0\n",
        "                    class_ids_total = []\n",
        "\n",
        "\n",
        "                    ################### Extending persons, obj scores to multiply with all pairs ######################\n",
        "\n",
        "                    for batch in range(len(pairs_info)):\n",
        "\n",
        "                        persons_score = []\n",
        "                        objects_score = []\n",
        "                        class_ids = []\n",
        "\n",
        "                        this_image = int(image_id[batch])\n",
        "\n",
        "                        scores_total = get_compact_detections(this_image, phase)\n",
        "                        persons_score, objects_score, persons_np, objects_np, class_ids = scores_total['person_bbx_score'], scores_total['object_bbx_score'], scores_total['person_bbx'], scores_total['objects_bbx'], scores_total['class_id_objects']\n",
        "\n",
        "                        objects_score.insert(0, float(1))\n",
        "\n",
        "\n",
        "                        temp_scores = extend(np.array(persons_score).reshape(len(persons_score), 1), int(pairs_info[batch][1]))\n",
        "                        persons_score_extended = np.concatenate([persons_score_extended, temp_scores])\n",
        "\n",
        "                        temp_scores = extend(persons_np, int(pairs_info[batch][1]))\n",
        "                        persons_np_extended = np.concatenate([persons_np_extended, temp_scores])\n",
        "\n",
        "                        temp_scores = extend_object(np.array(objects_score).reshape(len(objects_score), 1), int(pairs_info[batch][0]))\n",
        "                        objects_score_extended = np.concatenate([objects_score_extended, temp_scores])\n",
        "\n",
        "                        temp_scores = extend_object(objects_np, int(pairs_info[batch][0]))\n",
        "                        objects_np_extended = np.concatenate([objects_np_extended, temp_scores])\n",
        "\n",
        "                        temp_scores = extend_object(np.array(class_ids).reshape(len(class_ids),1),int(pairs_info[batch][0]))\n",
        "                        class_ids_extended = np.concatenate([class_ids_extended, temp_scores])\n",
        "                        class_ids_total.append(class_ids)\n",
        "\n",
        "\n",
        "\n",
        "                        start_pers += int(pairs_info[batch][0])\n",
        "                        start_obj += int(pairs_info[batch][1])\n",
        "                        start_tot = start_tot + int(pairs_info[batch][1]) * int(pairs_info[batch][0])\n",
        "\n",
        "\n",
        "                    ####################################################################################################\n",
        "\n",
        "                    ########## Applying LIS ##################################\n",
        "\n",
        "                    persons_score_extended = LIS(persons_score_extended, 8.3, 12, 10)\n",
        "                    objects_score_extended = LIS(objects_score_extended, 8.3, 12, 10)\n",
        "\n",
        "                    ###########################################################\n",
        "\n",
        "                    ########## Multiplying the scores from different streams along with the prior function from ican ###\n",
        "\n",
        "\n",
        "\n",
        "                    predicted_HOI = (predicted_HOI*predicted_HOI_combine*predicted_single*predicted_gem*persons_score_extended[1:]*objects_score_extended[1:])\n",
        "\n",
        "                    loss_mask = apply_prior(class_ids_extended[1:], predicted_HOI)\n",
        "                    predicted_HOI = loss_mask * predicted_HOI\n",
        "\n",
        "                    ######## Calculating loss ###################################################################\n",
        "\n",
        "                    N_b = min_batch_size * 29\n",
        "                    hum_obj_mask = torch.Tensor(objects_score_extended[1:]*persons_score_extended[1:]*loss_mask).to(device)\n",
        "\n",
        "                    lossf = torch.sum(loss_com_combine(sigmoid(outputs)*sigmoid(outputs_combine)*sigmoid(outputs_single)*hum_obj_mask*sigmoid(outputs_gem), labels.float())) / N_b\n",
        "\n",
        "                    lossc = lossf.item()\n",
        "\n",
        "                    acc_epoch += lossc\n",
        "                    iteration += 1\n",
        "\n",
        "                    if phase == 'train' or phase == 'val':\n",
        "                        lossf.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    #################################################################################################\n",
        "\n",
        "                    del lossf\n",
        "                    del model_out\n",
        "                    del inputs\n",
        "                    del outputs\n",
        "                    del labels\n",
        "\n",
        "                ########################### If visualization ######################################################\n",
        "\n",
        "                if visualize != 'f':\n",
        "                    visual(image_id, phase, pairs_info, predicted_HOI, predicted_single, objects_score_extended[1:], persons_score_extended[1:], predicted_HOI_combine, predicted_HOI_pair, true)\n",
        "\n",
        "                ###################################################################################################\n",
        "\n",
        "\n",
        "                ########## preparing for storing results #########################################################\n",
        "                predicted_scores = np.concatenate((predicted_scores, predicted_HOI), axis=0)\n",
        "                true_scores = np.concatenate((true_scores, true), axis=0)\n",
        "                predicted_scores_single = np.concatenate((predicted_scores_single, predicted_single), axis=0)\n",
        "                true_scores_single = np.concatenate((true_scores_single, true_single), axis=0)\n",
        "                ################################################################################################\n",
        "\n",
        "                ################### Storing the result in V-COCO format ########################################\n",
        "\n",
        "                if phase == 'test':\n",
        "\n",
        "                    if (epoch+1)%saving_epoch==0 or infr=='t':\n",
        "\n",
        "                        all_scores = filtering(predicted_HOI, true, persons_np_extended[1:], objects_np_extended[1:],predicted_single, pairs_info, image_id)\n",
        "\n",
        "                        infer_format(image_id, all_scores, phase, detections_test, pairs_info)\n",
        "\n",
        "                ###############################################################################################\n",
        "\n",
        "\n",
        "\n",
        "                ################# Breaking in particular number of epoch #####################################\n",
        "\n",
        "                #if iteration == break_point + 1:\n",
        "\n",
        "                    #break\n",
        "\n",
        "                ##############################################################################################\n",
        "\n",
        "            if phase == 'train':\n",
        "\n",
        "                loss_epoch_train.append((acc_epoch))\n",
        "\n",
        "                AP, AP_single = class_AP(predicted_scores[1:,:],true_scores[1:,:],predicted_scores_single[1:,],true_scores_single[1:,])\n",
        "\n",
        "                AP_train = pd.DataFrame(AP,columns =['Name_TRAIN', 'Score_TRAIN'])\n",
        "                AP_train_single = pd.DataFrame(AP_single,columns =['Name_TRAIN', 'Score_TRAIN'])\n",
        "\n",
        "            elif phase == 'val':\n",
        "\n",
        "                loss_epoch_val.append((acc_epoch))\n",
        "                AP,AP_single=class_AP(predicted_scores[1:,:],true_scores[1:,:],predicted_scores_single[1:,],true_scores_single[1:,])\n",
        "\n",
        "                AP_val = pd.DataFrame(AP,columns =['Name_VAL', 'Score_VAL'])\n",
        "                AP_val_single = pd.DataFrame(AP_single,columns =['Name_VAL', 'Score_VAL'])\n",
        "\n",
        "            elif phase == 'test':\n",
        "\n",
        "                loss_epoch_test.append((acc_epoch))\n",
        "                AP,AP_single=class_AP(predicted_scores[1:,:],true_scores[1:,:],predicted_scores_single[1:,],true_scores_single[1:,])\n",
        "                AP_test = pd.DataFrame(AP,columns =['Name_TEST', 'Score_TEST'])\n",
        "                AP_test_single = pd.DataFrame(AP_single,columns =['Name_TEST', 'Score_TEST'])\n",
        "\n",
        "                if (epoch+1)%saving_epoch==0 or infr=='t':\n",
        "                    file_name_p=folder_name+'/'+'test{}.pickle'.format(epoch+1)\n",
        "                    with open(file_name_p, 'wb') as handle:\n",
        "                        pickle.dump(detections_test, handle)\n",
        "\n",
        "\n",
        "\n",
        "        ##################################### Saving the model ########################################################\n",
        "\n",
        "        mean=AP_test.to_records(index=False)[29][1]\n",
        "\n",
        "        ##### Best Model ############\n",
        "\n",
        "        if mean>mean_best and infr!='t':\n",
        "\n",
        "            mean_best = mean\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'mean_best': mean_best,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler':scheduler.state_dict()\n",
        "            }, filename=folder_name+'/'+'bestcheckpoint.pth.tar')\n",
        "\n",
        "        ############################\n",
        "\n",
        "        if (epoch+1)%saving_epoch==0  and infr!='t':\n",
        "\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'mean_best': mean_best,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler':scheduler.state_dict()\n",
        "            }, filename=folder_name+'/'+str(epoch + 1 + 12)+'checkpoint.pth.tar')\n",
        "\n",
        "        ###########################\n",
        "\n",
        "        if infr=='t':\n",
        "            AP_final=pd.concat([AP_test],axis=1)\n",
        "            AP_final_single=pd.concat([AP_test_single],axis=1)\n",
        "            result.append(AP_final)\n",
        "            with open(file_name, 'wb') as handle:\n",
        "                pickle.dump(result, handle)\n",
        "\n",
        "        else:\n",
        "            AP_final=pd.concat([AP_train,AP_val,AP_test],axis=1)\n",
        "            AP_final_single=pd.concat([AP_train_single,AP_val_single,AP_test_single],axis=1)\n",
        "            result.append(AP_final)\n",
        "            with open(file_name, 'wb') as handle:\n",
        "                pickle.dump(result, handle)\n",
        "\n",
        "\n",
        "        time_elapsed = time.time() - initial_time_epoch\n",
        "        print('APs in EPOCH:{}'.format(epoch+1))\n",
        "        print(AP_final)\n",
        "        print(AP_final_single)\n",
        "\n",
        "        try:\n",
        "            print('Loss_train:{},Loss_validation:{},Loss_test:{}'.format(loss_epoch_train[epoch-start_epoch],loss_epoch_val[epoch-start_epoch],loss_epoch_test[epoch-start_epoch]))\n",
        "\n",
        "        except:\n",
        "            print('Loss_test:{}'.format(loss_epoch_test[epoch-start_epoch]))\n",
        "\n",
        "        print('This epoch completes in {:.0f}m {:.06f}s'.format(\n",
        "                        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "        if infr=='t':\n",
        "            break\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - initial_time\n",
        "    print('The whole process runs for {:.0f}h {:.0f}m {:0f}s'.format(time_elapsed //3600, (time_elapsed % 3600) // 60,((time_elapsed % 3600)%60)%60))\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLBCLPnJu32m"
      },
      "outputs": [],
      "source": [
        "####################### MAIN #######################################################\n",
        "\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def _init_fn(worker_id):\n",
        "    np.random.seed(int(seed))\n",
        "\n",
        "\n",
        "\n",
        "number_of_epochs = 1\n",
        "learning_rate = 0.01\n",
        "breaking_point = 10\n",
        "saving_epoch = 1\n",
        "first_word = \"/content/drive/MyDrive/HOI_Data/Checkpoints\"\n",
        "batch_size = 1\n",
        "resume_model = 't'\n",
        "infr = 't'\n",
        "hyp = 'f'\n",
        "visualize = 'f'\n",
        "check = \"best\"\n",
        "############################################\n",
        "\n",
        "all_data_dir = ALL_DET\n",
        "\n",
        "annotation_train = all_data_dir + 'Annotations_vcoco/train_annotations.json'\n",
        "image_dir_train = all_data_dir + 'Data_vcoco/train2014/'\n",
        "\n",
        "annotation_val = all_data_dir + 'Annotations_vcoco/val_annotations.json'\n",
        "image_dir_val = all_data_dir + 'Data_vcoco/train2014/'\n",
        "\n",
        "annotation_test = all_data_dir + 'Annotations_vcoco/test_annotations.json'\n",
        "image_dir_test = all_data_dir + 'Data_vcoco/val2014/'\n",
        "\n",
        "vcoco_train = vcoco_Dataset(annotation_train, image_dir_train, transform=transforms.Compose([Rescale((400, 400)), ToTensor()]))\n",
        "vcoco_val = vcoco_Dataset(annotation_val, image_dir_val, transform=transforms.Compose([Rescale((400, 400)), ToTensor()]))\n",
        "vcoco_test = vcoco_Dataset(annotation_test, image_dir_test, transform=transforms.Compose([Rescale((400, 400)), ToTensor()]))\n",
        "\n",
        "dataloader_train = DataLoader(vcoco_train, batch_size, shuffle=True,  collate_fn=vcoco_collate, num_workers=4, worker_init_fn=_init_fn)\n",
        "dataloader_val = DataLoader(vcoco_val, batch_size, shuffle=True, collate_fn=vcoco_collate, num_workers=4, worker_init_fn=_init_fn)\n",
        "dataloader_test = DataLoader(vcoco_test, batch_size, shuffle=False, collate_fn=vcoco_collate, num_workers=4, worker_init_fn=_init_fn)\n",
        "dataloader = {'train': dataloader_train, 'val': dataloader_val, 'test': dataloader_test}\n",
        "\n",
        "folder_name = '{}'.format(first_word)\n",
        "\n",
        "### Loading Model ###\n",
        "res = HOI_Detector()\n",
        "\n",
        "trainables = []\n",
        "not_trainables = []\n",
        "spmap = []\n",
        "single = []\n",
        "\n",
        "for name, p in res.named_parameters():\n",
        "    if name.split('.')[0] == 'Conv_pretrain':\n",
        "        p.requires_grad = False\n",
        "        not_trainables.append(p)\n",
        "    else:\n",
        "        if name.split('.')[0] in ['conv_sp_map', 'spmap_up']:\n",
        "            spmap.append(p)\n",
        "        else:\n",
        "            trainables.append(p)\n",
        "\n",
        "optim1 = optim.SGD(\n",
        "    [\n",
        "        {\"params\": trainables, \"lr\": learning_rate},\n",
        "        {\"params\": spmap, \"lr\": 0.001}\n",
        "    ],\n",
        "    momentum=0.9, weight_decay=0.0001\n",
        ")\n",
        "\n",
        "lambda1 = lambda epoch: 1.0 if epoch < 10 else (10 if epoch < 28 else 1)\n",
        "lambda2 = lambda epoch: 1\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optim1, [lambda1, lambda2])\n",
        "\n",
        "res.to(device)\n",
        "\n",
        "epoch = 0\n",
        "mean_best = 0\n",
        "\n",
        "if resume_model == 't':\n",
        "    try:\n",
        "        checkpoint = torch.load(folder_name + '/' + check + 'checkpoint.pth.tar')\n",
        "        res.load_state_dict(checkpoint['state_dict'], strict=True)\n",
        "        epoch = checkpoint['epoch']\n",
        "        mean_best = checkpoint['mean_best']\n",
        "        print(f\"=> loaded checkpoint when best_prediction mAP {mean_best} and epoch {checkpoint['epoch']}\")\n",
        "    except:\n",
        "        print('Failed to load checkpoint')\n",
        "\n",
        "if hyp == 't':\n",
        "    try:\n",
        "        print('Loading previous Hyperparameters')\n",
        "        optim1.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    except:\n",
        "        print('Failed to load previous Hyperparameters')\n",
        "\n",
        "train_test(res, optim1, scheduler, dataloader, number_of_epochs, breaking_point, saving_epoch, folder_name, batch_size, infr, epoch, mean_best, visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaYZKR1wxnY2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}